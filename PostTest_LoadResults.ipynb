{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PostTest_LoadResults.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPHzShvTlaUIPXHuOiSlUIH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachelRamirez/CIFAR-10/blob/main/PostTest_LoadResults.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCp4ve_TFY6N"
      },
      "source": [
        "# 5. Post-Test: Process Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHDNcf6KFeVE"
      },
      "source": [
        "After the 51 runs for the single groups have been run, its time to load the results into JMP to Fit the Definitive Screening Design and find which Main Effects have the most compeling effect.   \n",
        "\n",
        "* I need to get my pickle files unloaded into a dataframe to put in a CSV.\n",
        "\n",
        "* I had unintentionally created an effect that was captured by my blocking variable (early October?), but have since fixed it.  Original results here -- \"[GroupA_Test1](https://github.com/RachelRamirez/CIFAR-10/blob/main/CollectionofTests/Test1GroupA.ipynb)\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgGxW7Xi7a3M"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajvKCSvvs_ax"
      },
      "source": [
        "# ls /content/\n",
        "list_of_pickle_file_names = ['runs_GroupABCD_final.pkl'  ,\n",
        "  # I need to take part of EFGH and part of EFGH_final\n",
        "\n",
        "'runs_GroupAB_final.pkl',\n",
        "'runs_GroupE_final (1).pkl',\n",
        "'runs_GroupA_final (2).pkl'  ,\n",
        "'runs_GroupF_final (1).pkl',\n",
        "'runs_GroupB_final (7).pkl'  ,\n",
        "'runs_GroupG_final (1).pkl',\n",
        "'runs_GroupCD_final.pkl',\n",
        "'runs_GroupGH_final.pkl',\n",
        "'runs_GroupC_final (3).pkl' ,\n",
        "'runs_GroupH_final (1).pkl',\n",
        "'runs_GroupD_final (2).pkl'  ,\n",
        "'runs_GrouptrainABCDEFGH_final.pkl',\n",
        "'runs_GroupEF_final.pkl']\n",
        "\n",
        "list_of_group_names = ('ABCD'  ,\n",
        " # I need to take part of EFGH and part of EFGH_final\n",
        "'AB',\n",
        "'E',\n",
        "'A'  ,\n",
        "'F',\n",
        "'B'  ,\n",
        "'G',\n",
        "'CD',\n",
        "'GH',\n",
        "'C' ,\n",
        "'H',\n",
        "'D'  ,\n",
        "'ABCDEFGH',\n",
        "'EF')\n",
        "\n",
        "\n",
        "\n",
        "# for i in list_of_group_names\n",
        "# for run in range(smallestrun, lastmingrouprun+1): \n",
        "#   i = mingroupname\n",
        "#   cm = (ds[i][run][\"CM\"]).flatten(order=\"C\")\n",
        "#   cm = cm.flatten()\n",
        "#   cm_norm = (ds[i][run][\"CM_norm\"]).flatten(order=\"C\")\n",
        "#   cm_norm = cm_norm.flatten()\n",
        "#   # print(\"new\", cm_flat_new)\n",
        "#   row = [i, run,  ds[i][run][\"TestAccuracy\"],  ds[i][run][\"TestLoss\"], cm, cm_norm]\n",
        "#   rows.append(row)\n",
        "# for run in range(firstmaxgrouprun, biggestrun+1): \n",
        "#   i = maxgroupname\n",
        "#   cm = (ds[i][run][\"CM\"]).flatten(order=\"C\")\n",
        "#   cm = cm.flatten()\n",
        "#   cm_norm = (ds[i][run][\"CM_norm\"]).flatten(order=\"C\")\n",
        "#   cm_norm = cm_norm.flatten()\n",
        "#   # print(\"new\", cm_flat_new)\n",
        "#   row = [i, run,  ds[i][run][\"TestAccuracy\"],  ds[i][run][\"TestLoss\"], cm, cm_norm]\n",
        "#   rows.append(row)\n",
        "\n",
        "\n",
        " \n",
        "# df = pd.DataFrame(rows, columns=[\"Group\", \"Run\", \"TestAccuracy\", \"TestLoss\", \"CM\", \"CM_norm\"])\n",
        "\n",
        "# # df = df_Groups\n",
        "\n",
        "# CM_arr = np.array(df.loc[:,\"CM\"].to_list()) # this takes all rows in column \"CM\"\n",
        "# CM_df = pd.DataFrame(CM_arr, columns = \n",
        "                     \n",
        "#                      [\" 0 -> 0 \",\" 0 -> 1 \",\" 0 -> 2 \",\" 0 -> 3 \",\" 0 -> 4 \",\" 0 -> 5 \",\" 0 -> 6 \",\" 0 -> 7 \",\" 0 -> 8 \",\" 0 -> 9 \",\" 1 -> 0 \",\" 1 -> 1 \",\" 1 -> 2 \",\" 1 -> 3 \",\" 1 -> 4 \",\" 1 -> 5 \",\" 1 -> 6 \",\" 1 -> 7 \",\" 1 -> 8 \",\" 1 -> 9 \",\" 2 -> 0 \",\" 2 -> 1 \",\" 2 -> 2 \",\" 2 -> 3 \",\" 2 -> 4 \",\" 2 -> 5 \",\" 2 -> 6 \",\" 2 -> 7 \",\" 2 -> 8 \",\" 2 -> 9 \",\" 3 -> 0 \",\" 3 -> 1 \",\" 3 -> 2 \",\" 3 -> 3 \",\" 3 -> 4 \",\" 3 -> 5 \",\" 3 -> 6 \",\" 3 -> 7 \",\" 3 -> 8 \",\" 3 -> 9 \",\" 4 -> 0 \",\" 4 -> 1 \",\" 4 -> 2 \",\" 4 -> 3 \",\" 4 -> 4 \",\" 4 -> 5 \",\" 4 -> 6 \",\" 4 -> 7 \",\" 4 -> 8 \",\" 4 -> 9 \",\" 5 -> 0 \",\" 5 -> 1 \",\" 5 -> 2 \",\" 5 -> 3 \",\" 5 -> 4 \",\" 5 -> 5 \",\" 5 -> 6 \",\" 5 -> 7 \",\" 5 -> 8 \",\" 5 -> 9 \",\" 6 -> 0 \",\" 6 -> 1 \",\" 6 -> 2 \",\" 6 -> 3 \",\" 6 -> 4 \",\" 6 -> 5 \",\" 6 -> 6 \",\" 6 -> 7 \",\" 6 -> 8 \",\" 6 -> 9 \",\" 7 -> 0 \",\" 7 -> 1 \",\" 7 -> 2 \",\" 7 -> 3 \",\" 7 -> 4 \",\" 7 -> 5 \",\" 7 -> 6 \",\" 7 -> 7 \",\" 7 -> 8 \",\" 7 -> 9 \",\" 8 -> 0 \",\" 8 -> 1 \",\" 8 -> 2 \",\" 8 -> 3 \",\" 8 -> 4 \",\" 8 -> 5 \",\" 8 -> 6 \",\" 8 -> 7 \",\" 8 -> 8 \",\" 8 -> 9 \",\" 9 -> 0 \",\" 9 -> 1 \",\" 9 -> 2 \",\" 9 -> 3 \",\" 9 -> 4 \",\" 9 -> 5 \",\" 9 -> 6 \",\" 9 -> 7 \",\" 9 -> 8 \",\" 9 -> 9 \",]\n",
        "\n",
        "#                      ) #51*groups = rows, 100 = columns\n",
        "# CM_df = CM_df.reset_index(drop=True) #im not sure if this is necessary but its supposed to help when joining a DF to anoher DF\n",
        "# new_df = pd.concat([df, CM_df ], axis=1)\n",
        "\n",
        "# CM_norm_arr = np.array(df.loc[:,\"CM_norm\"].to_list()) # this takes all rows in column \"CM_norm\"\n",
        "# CM_norm_df = pd.DataFrame(CM_norm_arr, columns=\n",
        "#                           [\"0->0\\SumOfTrue0\" , \"0->1\\SumOfTrue0\" , \"0->2\\SumOfTrue0\" , \"0->3\\SumOfTrue0\" , \"0->4\\SumOfTrue0\" , \"0->5\\SumOfTrue0\" , \"0->6\\SumOfTrue0\" , \"0->7\\SumOfTrue0\" , \"0->8\\SumOfTrue0\" , \"0->9\\SumOfTrue0\" , \"1->0\\SumOfTrue1\" , \"1->1\\SumOfTrue1\" , \"1->2\\SumOfTrue1\" , \"1->3\\SumOfTrue1\" , \"1->4\\SumOfTrue1\" , \"1->5\\SumOfTrue1\" , \"1->6\\SumOfTrue1\" , \"1->7\\SumOfTrue1\" , \"1->8\\SumOfTrue1\" , \"1->9\\SumOfTrue1\" , \"2->0\\SumOfTrue2\" , \"2->1\\SumOfTrue2\" , \"2->2\\SumOfTrue2\" , \"2->3\\SumOfTrue2\" , \"2->4\\SumOfTrue2\" , \"2->5\\SumOfTrue2\" , \"2->6\\SumOfTrue2\" , \"2->7\\SumOfTrue2\" , \"2->8\\SumOfTrue2\" , \"2->9\\SumOfTrue2\" , \"3->0\\SumOfTrue3\" , \"3->1\\SumOfTrue3\" , \"3->2\\SumOfTrue3\" , \"3->3\\SumOfTrue3\" , \"3->4\\SumOfTrue3\" , \"3->5\\SumOfTrue3\" , \"3->6\\SumOfTrue3\" , \"3->7\\SumOfTrue3\" , \"3->8\\SumOfTrue3\" , \"3->9\\SumOfTrue3\" , \"4->0\\SumOfTrue4\" , \"4->1\\SumOfTrue4\" , \"4->2\\SumOfTrue4\" , \"4->3\\SumOfTrue4\" , \"4->4\\SumOfTrue4\" , \"4->5\\SumOfTrue4\" , \"4->6\\SumOfTrue4\" , \"4->7\\SumOfTrue4\" , \"4->8\\SumOfTrue4\" , \"4->9\\SumOfTrue4\" , \"5->0\\SumOfTrue5\" , \"5->1\\SumOfTrue5\" , \"5->2\\SumOfTrue5\" , \"5->3\\SumOfTrue5\" , \"5->4\\SumOfTrue5\" , \"5->5\\SumOfTrue5\" , \"5->6\\SumOfTrue5\" , \"5->7\\SumOfTrue5\" , \"5->8\\SumOfTrue5\" , \"5->9\\SumOfTrue5\" , \"6->0\\SumOfTrue6\" , \"6->1\\SumOfTrue6\" , \"6->2\\SumOfTrue6\" , \"6->3\\SumOfTrue6\" , \"6->4\\SumOfTrue6\" , \"6->5\\SumOfTrue6\" , \"6->6\\SumOfTrue6\" , \"6->7\\SumOfTrue6\" , \"6->8\\SumOfTrue6\" , \"6->9\\SumOfTrue6\" , \"7->0\\SumOfTrue7\" , \"7->1\\SumOfTrue7\" , \"7->2\\SumOfTrue7\" , \"7->3\\SumOfTrue7\" , \"7->4\\SumOfTrue7\" , \"7->5\\SumOfTrue7\" , \"7->6\\SumOfTrue7\" , \"7->7\\SumOfTrue7\" , \"7->8\\SumOfTrue7\" , \"7->9\\SumOfTrue7\" , \"8->0\\SumOfTrue8\" , \"8->1\\SumOfTrue8\" , \"8->2\\SumOfTrue8\" , \"8->3\\SumOfTrue8\" , \"8->4\\SumOfTrue8\" , \"8->5\\SumOfTrue8\" , \"8->6\\SumOfTrue8\" , \"8->7\\SumOfTrue8\" , \"8->8\\SumOfTrue8\" , \"8->9\\SumOfTrue8\" , \"9->0\\SumOfTrue9\" , \"9->1\\SumOfTrue9\" , \"9->2\\SumOfTrue9\" , \"9->3\\SumOfTrue9\" , \"9->4\\SumOfTrue9\" , \"9->5\\SumOfTrue9\" , \"9->6\\SumOfTrue9\" , \"9->7\\SumOfTrue9\" , \"9->8\\SumOfTrue9\" , \"9->9\\SumOfTrue9\" ]\n",
        "#                           ) \n",
        "# CM_norm_df = CM_norm_df.reset_index(drop=True)\n",
        " \n",
        "# # # Place the DataFrames side by side\n",
        "# # # horizontal_stack = pd.concat([survey_sub, survey_sub_last10], axis=1)\n",
        "\n",
        "\n",
        "# # #I want to remove the old CM and CM_Norm now that i verified they are joniing correctly\n",
        "# # test = new_df.drop(columns=[\"CM\", \"CM_norm\"])\n",
        "\n",
        "\n",
        "# new_df = pd.concat([new_df, CM_norm_df ], axis=1)\n",
        "# # new_df.to_csv('test.csv')\n",
        "# # I want to remove the old CM and CM_Norm now that i verified they are joniing correctly\n",
        "# new_df = new_df.drop(columns=[\"CM\", \"CM_norm\"])\n",
        "\n",
        "# new_df.to_csv('EFGH_only.csv')   # write results to a new csv file\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqzjaYlEM6Ug"
      },
      "source": [
        "as of 11/11/21 its  ** ~9 hours** to run   9 Groups with 51 Runs, at about 45 epochs each.  If I ran it again with GroupA (10th GRoup) it would probably be about 10.5 hours total to this point.  Also note that is just training and capturing results, not diplaying any images thru augmentations, as displaying images and counting the number in each batch is extremely time consuming.  I think it makes one experimental run take at least 20 minutes, so  20times longer overall."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXtNIneX-fbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0279d2-0d5f-45ff-fd58-5be3cd32852d"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "# what does a flattened confuion matrix look like \n",
        "\n",
        "\n",
        "# #define a list of the pickle files\n",
        "# list_of_pickle_file_names = ['runs_GroupAB_final.pkl', \n",
        "#                              'runs_GroupCD_final.pkl', \n",
        "#                              'runs_GroupEF_final.pkl', \n",
        "#                              'runs_GroupGH_final.pkl', \n",
        "#                           #   'runs_GroupEFGH.pkl', \n",
        "#                             ]\n",
        "#                             #  'runs_GroupG_final.pkl', \n",
        "#                             #  'runs_GroupH_final.pkl',] \n",
        "# #                             #  'runs_GroupI_final.pkl', \n",
        "# #                             #  'runs_GroupJ_final.pkl']\n",
        "\n",
        "# list_of_group_names = (\"AB\", \"CD\", \"EF\", \"GH\")    #, \"I\", \"J\")\n",
        "\n",
        "# # list_of_group_names = (\"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\")    #, \"I\", \"J\")\n",
        "\n",
        "list_of_pickle_file_names = ('runs_GroupEFGH (1).pkl', 'runs_GroupEFGH.pkl', 'runs_GroupEFGH_final.pkl')\n",
        "list_of_group_names = (\"EFGH_1\", \"EFGH_2\", \"EFGH_3\")\n",
        "\n",
        "ds = {}\n",
        "\n",
        "#for enumerate () zip the 9 pickle files:\n",
        "for Z, pkl, in zip(list_of_group_names, list_of_pickle_file_names): #list_of_unpickled_names):\n",
        "  \n",
        "  with open(pkl, 'rb') as fid:\n",
        "     print(\"dataset\", Z, \" loaded from\", pkl)\n",
        "     ds[Z] = pickle.load(fid)\n",
        " \n",
        "\n",
        "# #To reference a Group say ds[\"Letter\"][RunInt][\"Metric\"]\n",
        "# # The problem was i didnt have a I or J file with runs, nor some runs with 0\n",
        "# # ds[\"B\"][23][\"CM_norm\"]\n",
        "# # ds[\"C\"][23][\"CM_norm\"]\n",
        "\n",
        "# ds['B'][0][\"CM\"]  # the first array of ds[\"B\"][1][\"CM\"] is [551,  29,  37,  23,  45,  23,  37,  18, 199,  38],\n",
        "# ds['B'][1][\"CM\"]  # the first array of ds[\"B\"][1][\"CM\"] is [602,  54,  54,  19,  18,  17,  23,  19, 148,  46],\n",
        "\n",
        "# #can i just replace it Group , and each run, with this function? It'd be ice if i could map it.. versus a for loop of doomr\n",
        "rows = []\n",
        "\n",
        "firstmaxgrouprun = smallestrun = 50\n",
        "lastmingrouprun = biggestrun = 0\n",
        "mingroupname = ''\n",
        "maxgroupname = ''\n",
        "\n",
        "for i in (list_of_group_names):\n",
        "  firstrun = min(ds[i])\n",
        "  lastrun = max(ds[i])\n",
        "  if firstrun < smallestrun:\n",
        "    smallestrun = firstrun\n",
        "    mingroupname = i\n",
        "    lastmingrouprun = max(ds[mingroupname])\n",
        "  if lastrun > biggestrun:\n",
        "    biggestrun = lastrun\n",
        "    maxgroupname = i\n",
        "    firstmaxgrouprun = min(ds[maxgroupname])\n",
        "print(\"Name of  Group with earliest run number: \", mingroupname, \" smallest run:\", smallestrun, \", and last run: \", lastmingrouprun)  # <-- start with this group and run\n",
        "print(\"Name of  Group with max run number: \", maxgroupname, \" starts at\", firstmaxgrouprun, \" ends at\",  biggestrun)   # <-- end with this group and run\n",
        "\n",
        "\n",
        "# Name of  Group with earliest run number:  EFGH_2  smallest run: 0 , and last run:  24\n",
        "# Name of  Group with max run number:  EFGH_3  starts at 25  ends at 50\n",
        "# so i want to take efgh_2 from 0 to 24 and load that\n",
        "# and then i want to take efgh_3 from 25 to 50\n",
        "\n",
        "for run in range(smallestrun, lastmingrouprun+1): \n",
        "  i = mingroupname\n",
        "  cm = (ds[i][run][\"CM\"]).flatten(order=\"C\")\n",
        "  cm = cm.flatten()\n",
        "  cm_norm = (ds[i][run][\"CM_norm\"]).flatten(order=\"C\")\n",
        "  cm_norm = cm_norm.flatten()\n",
        "  # print(\"new\", cm_flat_new)\n",
        "  row = [i, run,  ds[i][run][\"TestAccuracy\"],  ds[i][run][\"TestLoss\"], cm, cm_norm]\n",
        "  rows.append(row)\n",
        "for run in range(firstmaxgrouprun, biggestrun+1): \n",
        "  i = maxgroupname\n",
        "  cm = (ds[i][run][\"CM\"]).flatten(order=\"C\")\n",
        "  cm = cm.flatten()\n",
        "  cm_norm = (ds[i][run][\"CM_norm\"]).flatten(order=\"C\")\n",
        "  cm_norm = cm_norm.flatten()\n",
        "  # print(\"new\", cm_flat_new)\n",
        "  row = [i, run,  ds[i][run][\"TestAccuracy\"],  ds[i][run][\"TestLoss\"], cm, cm_norm]\n",
        "  rows.append(row)\n",
        "\n",
        "\n",
        " \n",
        "df = pd.DataFrame(rows, columns=[\"Group\", \"Run\", \"TestAccuracy\", \"TestLoss\", \"CM\", \"CM_norm\"])\n",
        "\n",
        "# df = df_Groups\n",
        "\n",
        "CM_arr = np.array(df.loc[:,\"CM\"].to_list()) # this takes all rows in column \"CM\"\n",
        "CM_df = pd.DataFrame(CM_arr, columns = \n",
        "                     \n",
        "                     [\" 0 -> 0 \",\" 0 -> 1 \",\" 0 -> 2 \",\" 0 -> 3 \",\" 0 -> 4 \",\" 0 -> 5 \",\" 0 -> 6 \",\" 0 -> 7 \",\" 0 -> 8 \",\" 0 -> 9 \",\" 1 -> 0 \",\" 1 -> 1 \",\" 1 -> 2 \",\" 1 -> 3 \",\" 1 -> 4 \",\" 1 -> 5 \",\" 1 -> 6 \",\" 1 -> 7 \",\" 1 -> 8 \",\" 1 -> 9 \",\" 2 -> 0 \",\" 2 -> 1 \",\" 2 -> 2 \",\" 2 -> 3 \",\" 2 -> 4 \",\" 2 -> 5 \",\" 2 -> 6 \",\" 2 -> 7 \",\" 2 -> 8 \",\" 2 -> 9 \",\" 3 -> 0 \",\" 3 -> 1 \",\" 3 -> 2 \",\" 3 -> 3 \",\" 3 -> 4 \",\" 3 -> 5 \",\" 3 -> 6 \",\" 3 -> 7 \",\" 3 -> 8 \",\" 3 -> 9 \",\" 4 -> 0 \",\" 4 -> 1 \",\" 4 -> 2 \",\" 4 -> 3 \",\" 4 -> 4 \",\" 4 -> 5 \",\" 4 -> 6 \",\" 4 -> 7 \",\" 4 -> 8 \",\" 4 -> 9 \",\" 5 -> 0 \",\" 5 -> 1 \",\" 5 -> 2 \",\" 5 -> 3 \",\" 5 -> 4 \",\" 5 -> 5 \",\" 5 -> 6 \",\" 5 -> 7 \",\" 5 -> 8 \",\" 5 -> 9 \",\" 6 -> 0 \",\" 6 -> 1 \",\" 6 -> 2 \",\" 6 -> 3 \",\" 6 -> 4 \",\" 6 -> 5 \",\" 6 -> 6 \",\" 6 -> 7 \",\" 6 -> 8 \",\" 6 -> 9 \",\" 7 -> 0 \",\" 7 -> 1 \",\" 7 -> 2 \",\" 7 -> 3 \",\" 7 -> 4 \",\" 7 -> 5 \",\" 7 -> 6 \",\" 7 -> 7 \",\" 7 -> 8 \",\" 7 -> 9 \",\" 8 -> 0 \",\" 8 -> 1 \",\" 8 -> 2 \",\" 8 -> 3 \",\" 8 -> 4 \",\" 8 -> 5 \",\" 8 -> 6 \",\" 8 -> 7 \",\" 8 -> 8 \",\" 8 -> 9 \",\" 9 -> 0 \",\" 9 -> 1 \",\" 9 -> 2 \",\" 9 -> 3 \",\" 9 -> 4 \",\" 9 -> 5 \",\" 9 -> 6 \",\" 9 -> 7 \",\" 9 -> 8 \",\" 9 -> 9 \",]\n",
        "\n",
        "                     ) #51*groups = rows, 100 = columns\n",
        "CM_df = CM_df.reset_index(drop=True) #im not sure if this is necessary but its supposed to help when joining a DF to anoher DF\n",
        "new_df = pd.concat([df, CM_df ], axis=1)\n",
        "\n",
        "CM_norm_arr = np.array(df.loc[:,\"CM_norm\"].to_list()) # this takes all rows in column \"CM_norm\"\n",
        "CM_norm_df = pd.DataFrame(CM_norm_arr, columns=\n",
        "                          [\"0->0\\SumOfTrue0\" , \"0->1\\SumOfTrue0\" , \"0->2\\SumOfTrue0\" , \"0->3\\SumOfTrue0\" , \"0->4\\SumOfTrue0\" , \"0->5\\SumOfTrue0\" , \"0->6\\SumOfTrue0\" , \"0->7\\SumOfTrue0\" , \"0->8\\SumOfTrue0\" , \"0->9\\SumOfTrue0\" , \"1->0\\SumOfTrue1\" , \"1->1\\SumOfTrue1\" , \"1->2\\SumOfTrue1\" , \"1->3\\SumOfTrue1\" , \"1->4\\SumOfTrue1\" , \"1->5\\SumOfTrue1\" , \"1->6\\SumOfTrue1\" , \"1->7\\SumOfTrue1\" , \"1->8\\SumOfTrue1\" , \"1->9\\SumOfTrue1\" , \"2->0\\SumOfTrue2\" , \"2->1\\SumOfTrue2\" , \"2->2\\SumOfTrue2\" , \"2->3\\SumOfTrue2\" , \"2->4\\SumOfTrue2\" , \"2->5\\SumOfTrue2\" , \"2->6\\SumOfTrue2\" , \"2->7\\SumOfTrue2\" , \"2->8\\SumOfTrue2\" , \"2->9\\SumOfTrue2\" , \"3->0\\SumOfTrue3\" , \"3->1\\SumOfTrue3\" , \"3->2\\SumOfTrue3\" , \"3->3\\SumOfTrue3\" , \"3->4\\SumOfTrue3\" , \"3->5\\SumOfTrue3\" , \"3->6\\SumOfTrue3\" , \"3->7\\SumOfTrue3\" , \"3->8\\SumOfTrue3\" , \"3->9\\SumOfTrue3\" , \"4->0\\SumOfTrue4\" , \"4->1\\SumOfTrue4\" , \"4->2\\SumOfTrue4\" , \"4->3\\SumOfTrue4\" , \"4->4\\SumOfTrue4\" , \"4->5\\SumOfTrue4\" , \"4->6\\SumOfTrue4\" , \"4->7\\SumOfTrue4\" , \"4->8\\SumOfTrue4\" , \"4->9\\SumOfTrue4\" , \"5->0\\SumOfTrue5\" , \"5->1\\SumOfTrue5\" , \"5->2\\SumOfTrue5\" , \"5->3\\SumOfTrue5\" , \"5->4\\SumOfTrue5\" , \"5->5\\SumOfTrue5\" , \"5->6\\SumOfTrue5\" , \"5->7\\SumOfTrue5\" , \"5->8\\SumOfTrue5\" , \"5->9\\SumOfTrue5\" , \"6->0\\SumOfTrue6\" , \"6->1\\SumOfTrue6\" , \"6->2\\SumOfTrue6\" , \"6->3\\SumOfTrue6\" , \"6->4\\SumOfTrue6\" , \"6->5\\SumOfTrue6\" , \"6->6\\SumOfTrue6\" , \"6->7\\SumOfTrue6\" , \"6->8\\SumOfTrue6\" , \"6->9\\SumOfTrue6\" , \"7->0\\SumOfTrue7\" , \"7->1\\SumOfTrue7\" , \"7->2\\SumOfTrue7\" , \"7->3\\SumOfTrue7\" , \"7->4\\SumOfTrue7\" , \"7->5\\SumOfTrue7\" , \"7->6\\SumOfTrue7\" , \"7->7\\SumOfTrue7\" , \"7->8\\SumOfTrue7\" , \"7->9\\SumOfTrue7\" , \"8->0\\SumOfTrue8\" , \"8->1\\SumOfTrue8\" , \"8->2\\SumOfTrue8\" , \"8->3\\SumOfTrue8\" , \"8->4\\SumOfTrue8\" , \"8->5\\SumOfTrue8\" , \"8->6\\SumOfTrue8\" , \"8->7\\SumOfTrue8\" , \"8->8\\SumOfTrue8\" , \"8->9\\SumOfTrue8\" , \"9->0\\SumOfTrue9\" , \"9->1\\SumOfTrue9\" , \"9->2\\SumOfTrue9\" , \"9->3\\SumOfTrue9\" , \"9->4\\SumOfTrue9\" , \"9->5\\SumOfTrue9\" , \"9->6\\SumOfTrue9\" , \"9->7\\SumOfTrue9\" , \"9->8\\SumOfTrue9\" , \"9->9\\SumOfTrue9\" ]\n",
        "                          ) \n",
        "CM_norm_df = CM_norm_df.reset_index(drop=True)\n",
        " \n",
        "# # Place the DataFrames side by side\n",
        "# # horizontal_stack = pd.concat([survey_sub, survey_sub_last10], axis=1)\n",
        "\n",
        "\n",
        "# #I want to remove the old CM and CM_Norm now that i verified they are joniing correctly\n",
        "# test = new_df.drop(columns=[\"CM\", \"CM_norm\"])\n",
        "\n",
        "\n",
        "new_df = pd.concat([new_df, CM_norm_df ], axis=1)\n",
        "# new_df.to_csv('test.csv')\n",
        "# I want to remove the old CM and CM_Norm now that i verified they are joniing correctly\n",
        "new_df = new_df.drop(columns=[\"CM\", \"CM_norm\"])\n",
        "\n",
        "new_df.to_csv('EFGH_only.csv')   # write results to a new csv file\n",
        "\n",
        " "
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset EFGH_1  loaded from runs_GroupEFGH (1).pkl\n",
            "dataset EFGH_2  loaded from runs_GroupEFGH.pkl\n",
            "dataset EFGH_3  loaded from runs_GroupEFGH_final.pkl\n",
            "Name of  Group with earliest run number:  EFGH_2  smallest run: 0 , and last run:  24\n",
            "Name of  Group with max run number:  EFGH_3  starts at 25  ends at 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nQ52WxYg1KW"
      },
      "source": [
        "array([[602,  54,  54, ...,  36,  89, 585],  run0\n",
        "       [559,  37,  67, ...,  21,  95, 633],  run1 \n",
        "       [393,  29,  45, ...,  40, 110, 650],\n",
        "       ...,\n",
        "       [545,  54,  76, ...,  31,  87, 579],\n",
        "       [462,  56,  34, ...,  27, 155, 610],\n",
        "       [347,  43,  70, ...,  53, 120, 564]])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUxelvZjNYSb",
        "outputId": "64f4f2e6-4184-40db-fca1-69253bf45aff"
      },
      "source": [
        "\n",
        "# # https://stackoverflow.com/questions/67125762/convert-an-array-column-into-multiple-columns-python\n",
        "\n",
        "# # You can convert your dataframe in this way:\n",
        "# df1 = pd.DataFrame({0:[[2387, 1098], [1873, 6792],], 1:[0,1]})\n",
        "# arr = np.array(df.loc[:,0].to_list())\n",
        "# df2 = pd.DataFrame({0:arr[:,0], 1:arr[:,1], 2:df.loc[:,1]})\n",
        "# print(df2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0     1  2\n",
            "0  2387  1098  0\n",
            "1  1873  6792  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZdFrjSd9hF0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKp0O_WzHLW-"
      },
      "source": [
        "I only have **'runs_GroupTrainA_Zipped_final.pkl'** so I'm going to manually open that and put in CSV format for JMP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "mVTtOc7UM7WG",
        "outputId": "791b48cf-5ea4-4243-d48f-fc018fb9536d"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "#load one file manually\n",
        "\n",
        "ds={}\n",
        "\n",
        "\n",
        "filename = 'runs_GroupEFGH (1).pkl'\n",
        "letter = \"A\"\n",
        "with open(filename, 'rb') as fid:\n",
        "    print(\"dataset\",letter, \" loaded from\", filename)\n",
        "    ds[letter] = pickle.load(fid)\n",
        "\n",
        "print(\"What is in ds[letter]:\", ds[letter])\n",
        "\n",
        "print(\"Latest error is I cannot flatten ds[letter][run][\\\"CM\\\"], because type(ds[letter][run][\\\"CM\\\"]) is a list\")\n",
        "\n",
        "ds[letter][run][\"CM\"]\n",
        "\n",
        "rows = [] \n",
        "for run in range(0,51,50):             #<!!---------------- ------ change back to 0 once you have and/or end with 51\n",
        "  cm = np.array(ds[letter][run][\"CM\"]).flatten(order=\"C\")\n",
        "  cm = cm.flatten()\n",
        "  cm_norm = (ds[letter][run][\"CM_norm\"]) #.flatten(order=\"C\")\n",
        "  cm_norm = cm_norm.flatten()\n",
        "  # print(\"new\", cm_flat_new)\n",
        "  row = [letter, run,  ds[letter][run][\"TestAccuracy\"], ds[letter][run][\"TestLoss\"], cm, cm_norm]\n",
        "  rows.append(row)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\"Group\", \"Run\", \"TestAccuracy\", \"TestLoss\", \"CM\", \"CM_norm\"])\n",
        " \n",
        "# JOIN the two dataframe tables together, to allow CM to take up 100 columnns \n",
        "CM_arr = np.array(df.loc[:,\"CM\"].to_list()) # this takes all 51 rows in column \"CM\"\n",
        "CM_df = pd.DataFrame(CM_arr) #51 rows 100 columns\n",
        "CM_df = CM_df.reset_index(drop=True)  # I'm not sure if i need this but it helps when joining to ensure no mismatch between indices\n",
        "new_df = pd.concat([df, CM_df ], axis=1)  #once concatenated, i call the new df new_df\n",
        "\n",
        "\n",
        "# Create the CM_Norm dataframe of 100 columns\n",
        "CM_norm_arr = np.array(df.loc[:,\"CM_norm\"].to_list()) # this takes all rows in column \"CM\"\n",
        "CM_norm_df = pd.DataFrame(CM_norm_arr) #51 rows 100 columns\n",
        "CM_norm_df = CM_norm_df.reset_index(drop=True)\n",
        "\n",
        "# Place the DataFrames side by side\n",
        "# horizontal_stack = pd.concat([survey_sub, survey_sub_last10], axis=1)\n",
        "\n",
        "new_df = pd.concat([new_df, CM_norm_df ], axis=1)\n",
        "\n",
        "#I want to remove the old CM and CM_Norm now that i verified they are joniing correctly\n",
        "test = new_df.drop(columns=[\"CM\", \"CM_norm\"])\n",
        "\n",
        "\n",
        "csv_filename = str(filename) + str(\".csv\")\n",
        "new_df.to_csv()   # write results to a new csv file\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-c5c48cc92714>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'runs_GroupA_final (1).pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mletter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"A\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mletter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" loaded from\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mletter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'runs_GroupA_final (1).pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75ahaoL_3my4"
      },
      "source": [
        "im going to make column headers for the 0-99 CM columns and the 0-99 CM_norm columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf_gEQWf3u2t",
        "outputId": "6a0c6950-3e2d-484c-a254-3bea050c8d4f"
      },
      "source": [
        "# for i in range(0,10):\n",
        "#   for j in range(0,10):\n",
        "#     print( \"\\\"\" ,   i,\"->\",j,        \"\\\",\",  end=\"\" )\n",
        "\n",
        "for i in range(0,10):\n",
        "   for j in range(0,10):\n",
        "     print( \"\\\"\" ,   i,\"->\",j,   \"\\SumOfTrue\" , i,  \"\\\" , \",  sep=\"\", end=\"\" )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"0->0\\SumOfTrue0\" , \"0->1\\SumOfTrue0\" , \"0->2\\SumOfTrue0\" , \"0->3\\SumOfTrue0\" , \"0->4\\SumOfTrue0\" , \"0->5\\SumOfTrue0\" , \"0->6\\SumOfTrue0\" , \"0->7\\SumOfTrue0\" , \"0->8\\SumOfTrue0\" , \"0->9\\SumOfTrue0\" , \"1->0\\SumOfTrue1\" , \"1->1\\SumOfTrue1\" , \"1->2\\SumOfTrue1\" , \"1->3\\SumOfTrue1\" , \"1->4\\SumOfTrue1\" , \"1->5\\SumOfTrue1\" , \"1->6\\SumOfTrue1\" , \"1->7\\SumOfTrue1\" , \"1->8\\SumOfTrue1\" , \"1->9\\SumOfTrue1\" , \"2->0\\SumOfTrue2\" , \"2->1\\SumOfTrue2\" , \"2->2\\SumOfTrue2\" , \"2->3\\SumOfTrue2\" , \"2->4\\SumOfTrue2\" , \"2->5\\SumOfTrue2\" , \"2->6\\SumOfTrue2\" , \"2->7\\SumOfTrue2\" , \"2->8\\SumOfTrue2\" , \"2->9\\SumOfTrue2\" , \"3->0\\SumOfTrue3\" , \"3->1\\SumOfTrue3\" , \"3->2\\SumOfTrue3\" , \"3->3\\SumOfTrue3\" , \"3->4\\SumOfTrue3\" , \"3->5\\SumOfTrue3\" , \"3->6\\SumOfTrue3\" , \"3->7\\SumOfTrue3\" , \"3->8\\SumOfTrue3\" , \"3->9\\SumOfTrue3\" , \"4->0\\SumOfTrue4\" , \"4->1\\SumOfTrue4\" , \"4->2\\SumOfTrue4\" , \"4->3\\SumOfTrue4\" , \"4->4\\SumOfTrue4\" , \"4->5\\SumOfTrue4\" , \"4->6\\SumOfTrue4\" , \"4->7\\SumOfTrue4\" , \"4->8\\SumOfTrue4\" , \"4->9\\SumOfTrue4\" , \"5->0\\SumOfTrue5\" , \"5->1\\SumOfTrue5\" , \"5->2\\SumOfTrue5\" , \"5->3\\SumOfTrue5\" , \"5->4\\SumOfTrue5\" , \"5->5\\SumOfTrue5\" , \"5->6\\SumOfTrue5\" , \"5->7\\SumOfTrue5\" , \"5->8\\SumOfTrue5\" , \"5->9\\SumOfTrue5\" , \"6->0\\SumOfTrue6\" , \"6->1\\SumOfTrue6\" , \"6->2\\SumOfTrue6\" , \"6->3\\SumOfTrue6\" , \"6->4\\SumOfTrue6\" , \"6->5\\SumOfTrue6\" , \"6->6\\SumOfTrue6\" , \"6->7\\SumOfTrue6\" , \"6->8\\SumOfTrue6\" , \"6->9\\SumOfTrue6\" , \"7->0\\SumOfTrue7\" , \"7->1\\SumOfTrue7\" , \"7->2\\SumOfTrue7\" , \"7->3\\SumOfTrue7\" , \"7->4\\SumOfTrue7\" , \"7->5\\SumOfTrue7\" , \"7->6\\SumOfTrue7\" , \"7->7\\SumOfTrue7\" , \"7->8\\SumOfTrue7\" , \"7->9\\SumOfTrue7\" , \"8->0\\SumOfTrue8\" , \"8->1\\SumOfTrue8\" , \"8->2\\SumOfTrue8\" , \"8->3\\SumOfTrue8\" , \"8->4\\SumOfTrue8\" , \"8->5\\SumOfTrue8\" , \"8->6\\SumOfTrue8\" , \"8->7\\SumOfTrue8\" , \"8->8\\SumOfTrue8\" , \"8->9\\SumOfTrue8\" , \"9->0\\SumOfTrue9\" , \"9->1\\SumOfTrue9\" , \"9->2\\SumOfTrue9\" , \"9->3\\SumOfTrue9\" , \"9->4\\SumOfTrue9\" , \"9->5\\SumOfTrue9\" , \"9->6\\SumOfTrue9\" , \"9->7\\SumOfTrue9\" , \"9->8\\SumOfTrue9\" , \"9->9\\SumOfTrue9\" , "
          ]
        }
      ]
    }
  ]
}